# -*- coding: utf-8 -*-
"""pyspark_pokemon_movies.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fzRRYfkQrlgQ2Ql47h-ZFgaIhPjKbh0g
"""

!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q https://www-us.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz
!tar xf spark-2.4.4-bin-hadoop2.7.tgz
!pip install -q findspark

from google.colab import files
files.upload()

#DATASET LINKS
### https://www.kaggle.com/ranja7/movieratingsbyusers
### https://www.kaggle.com/rounakbanik/pokemon

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-2.4.4-bin-hadoop2.7"

import findspark
findspark.init()

"""#### Libraries"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from pyspark import SparkConf, SparkContext
from pyspark.sql import SQLContext, SparkSession
from pyspark.sql.functions import col, isnan, count, when

from pyspark.ml.clustering import KMeans
from pyspark.mllib.recommendation import ALS
from pyspark.mllib.classification import LabeledPoint, LogisticRegressionWithLBFGS

from pyspark.ml.feature import VectorAssembler
from pyspark.ml.evaluation import ClusteringEvaluator

"""#### Configuration"""

conf = SparkConf().setAppName('App').setMaster('local')
sc = SparkContext(conf=conf)
spark = SparkSession(sc)

# Loading Data from CSV file. Pokemon Data
dataset = spark.read.csv("pokemon.csv", header=True, inferSchema=True)
dataset.show(7)

"""### Exploratory Data Analysis"""

# Rows & Columns of Data
print ("Rows:", dataset.count())
print ("Columns:", len(dataset.columns))

# Null Values
dataset.select([count(when(col(cl).isNull() | isnan(cl), cl)).alias(cl) for cl in dataset.columns]).show()

# Pandas Dataframe
df = dataset.toPandas()

# hp: The Base HP of the Pokemon
plt.hist(df["hp"], bins=20, edgecolor="black", alpha=0.5)

# "defense: The Base Defense of the Pokemon"
plt.hist(df["defense"], bins=20, edgecolor="black", alpha=0.5)

# "attack: The Base Attack of the Pokemon"
plt.hist(df["attack"], bins=30, edgecolor="black", alpha=0.5)

# Classes & their Counts
classes, counts = np.unique(np.array(df["is_legendary"]), return_counts=True)
print(classes, counts)

# Balanced Class
plt.bar(range(len(counts)), counts, edgecolor="black", alpha=0.5)

"""### Recommender System
##### Collaborative filtering using Alternative Least Squares Method
"""

# Loading Data
dataset = spark.read.csv("ratings_user-item.csv", header=True)
dataset.show(10)
dataset.count()

# NOTE: This Cell sometimes gives an error, so run it once again and it will run without error..

# Train & Test Data
df = dataset.toPandas()
train = []
test = []
for idx, row in df.iterrows():
    train.append((row["userId"], row["movieId"], row["rating"]))
    test.append((row["userId"], row["movieId"]))
    

# Training Model
rdd_train = sc.parallelize(train)
model = ALS.train(rdd_train, 1, seed=50)

# Testing Model
rdd_test = sc.parallelize(test)
test_preds = model.predictAll(rdd_test)

test_preds = spark.createDataFrame(test_preds)
test_preds.createOrReplaceTempView("Prediction")
test_preds = test_preds.toPandas()
test_ratings = test_preds["rating"]
test_ratings = list(test_ratings)

# Evaluating Model with Mean Square Error
error = 0.0
for x in range(len(test_ratings)):
    error += (test_ratings[x] - float(train[x][2])) ** 2
error /= len(test_ratings)

print("Mean Square Error(MSE) =", error)

"""### Classification
##### Logistic regression using LogisticRegressionWithLBFGS Method
"""

# Loading Data
dataset = spark.read.csv("pokemon.csv", header=True, inferSchema=True)
dataset.show(5)

# Train & Test Data
df = dataset.toPandas()
train = []
test = []
for i, row in df.iterrows():
    feats = [row["hp"], row["attack"], row["defense"], row["generation"]]
    target_label = row["is_legendary"]

    train.append(LabeledPoint(target_label, feats))
    test.append(feats)

# Training
rdd_train = sc.parallelize(train)
model = LogisticRegressionWithLBFGS.train(rdd_train, iterations=10, numClasses=len(counts))

# Testing/Predicting
rdd_test = sc.parallelize(test)
test_preds = model.predict(rdd_test)
test_preds = test_preds.map(lambda x: x).collect()

# Evaluating Model with Accuracy
x = 0
for x in range(len(train)):
    if int(test_preds[x]) == int(train[x].label):
        x += 1
print("Accuracy =", (x / len(train)))



"""### Clustering
##### K-Means Clustering
"""

select_cols = ["hp","defense","attack", "base_happiness"]
vec_ass = VectorAssembler(inputCols=select_cols, outputCol="features").transform(dataset)
# vec_ass.show()
# Training
model = KMeans(k=len(classes), seed=50).fit(vec_ass.select('features'))

# Testing
test_preds = model.transform(vec_ass)

# Evaluating Model with ClusteringEvaluator
print("Accuracy =", ClusteringEvaluator(predictionCol="prediction").evaluate(test_preds))

sc.stop()